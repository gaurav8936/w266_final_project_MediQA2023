{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a345bf3-d1e2-4d9f-98a5-0826aead27cd",
   "metadata": {},
   "source": [
    "# <font color = orange> Section Header --> <font color = teal> Redo Label Dictionary Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f94e26-11f2-4e54-8003-6ed514a61a47",
   "metadata": {},
   "source": [
    "### <font color = tomato> roberta-base XL --> label dict OLD & NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdef224-54f7-4016-ac6c-882829f592b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2af10e8-a3c9-41a7-9d22-e64750283d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv.main import load_dotenv\n",
    "from datasets import load_dataset, load_metric\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b2e822-6513-4bda-9c4b-6459c53a246a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02411b09-4104-4e1d-a056-d5c8cf6d92e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/20231116_MediQA/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "# Set to display full (non-truncated) dataframe information\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc3f1cc-cf4e-4020-96b1-90bab854d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# Set your Hugging Face API token as an environment variable\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Save the token using HfFolder\n",
    "HfFolder.save_token(os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921ed369-cc67-4a7b-88f4-cbeef98b2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed_value = 1234\n",
    "\n",
    "random.seed(seed_value)  # Python\n",
    "np.random.seed(seed_value)  # Numpy\n",
    "torch.manual_seed(seed_value)  # PyTorch\n",
    "\n",
    "# If using CUDA\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)  # for multi-GPU\n",
    "\n",
    "# Additionally, for reproducibility in PyTorch, you might want to add:\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91ab85-9ecd-4be2-be61-1d860e5b31a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e962e4-b217-4b21-af49-cdc507c4b777",
   "metadata": {},
   "source": [
    "## <font color = Goldenrod> Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f6c6482-01d5-49a6-8838-4f65e848c9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 21:01:12.059747: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 21:01:12.059791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 21:01:12.059813: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44c312-bba9-4b3c-a42b-ab9228dcbfa2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e84d47-083d-40d2-8abb-bb69dc6236dd",
   "metadata": {},
   "source": [
    "# <font color = blue> Part 1: Prepare Training/Validation/Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee5922-918a-4726-a03c-a5cc47292b32",
   "metadata": {},
   "source": [
    "## <font color = Goldenrod> Step 1: Load and Prepare the Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c39cd3-1d63-43a7-afb3-261a71908c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Function to load dataset\n",
    "def load_dataset(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data[['dialogue', 'section_header']]\n",
    "    data.columns = [\"text\", \"labels\"]\n",
    "    return data\n",
    "\n",
    "# Function to get unique labels from a dataset\n",
    "def get_unique_labels(data):\n",
    "    return data['labels'].unique()\n",
    "\n",
    "# Load datasets\n",
    "train_data = load_dataset('/home/gaurav_narasimhan/03.gn_projects/03.MediQA_2023/11.Source_Data/TaskA-TrainingSet.csv')\n",
    "val_data = load_dataset('/home/gaurav_narasimhan/03.gn_projects/03.MediQA_2023/11.Source_Data/TaskA-ValidationSet.csv')\n",
    "test_data = load_dataset('/home/gaurav_narasimhan/03.gn_projects/03.MediQA_2023/11.Source_Data/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv')\n",
    "\n",
    "# Get unique labels from both datasets\n",
    "unique_labels = np.unique(np.concatenate([get_unique_labels(train_data), get_unique_labels(val_data), get_unique_labels(test_data)]))\n",
    "\n",
    "# Create unified label dictionary\n",
    "# label_dict = {label: idx for idx, label in enumerate(unique_labels)} # ENUMERATE IS WHAT F***ED THIS UP\n",
    "# label_dict = {\n",
    "#     'GENHX': 0,\n",
    "#     'MEDICATIONS': 1,\n",
    "#     'CC': 2,\n",
    "#     'PASTMEDICALHX': 3,\n",
    "#     'ALLERGY': 4,\n",
    "#     'FAM/SOCHX': 5,\n",
    "#     'PASTSURGICAL': 6,\n",
    "#     'OTHER_HISTORY': 7,\n",
    "#     'ASSESSMENT': 8,\n",
    "#     'ROS': 9,\n",
    "#     'DISPOSITION': 10,\n",
    "#     'EXAM': 11,\n",
    "#     'PLAN': 12,\n",
    "#     'DIAGNOSIS': 13,\n",
    "#     'EDCOURSE': 14,\n",
    "#     'IMMUNIZATIONS': 15,\n",
    "#     'LABS': 16,\n",
    "#     'IMAGING': 17,\n",
    "#     'PROCEDURES': 18,\n",
    "#     'GYNHX': 19\n",
    "# }\n",
    "\n",
    "label_dict = {\n",
    "    'ALLERGY': 0,\n",
    "    'ASSESSMENT': 1,\n",
    "    'CC': 2,\n",
    "    'DIAGNOSIS': 3,\n",
    "    'DISPOSITION': 4,\n",
    "    'EDCOURSE': 5,\n",
    "    'EXAM': 6,\n",
    "    'FAM/SOCHX': 7,\n",
    "    'GENHX': 8,\n",
    "    'GYNHX': 9,\n",
    "    'IMAGING': 10,\n",
    "    'IMMUNIZATIONS': 11,\n",
    "    'LABS': 12,\n",
    "    'MEDICATIONS': 13,\n",
    "    'OTHER_HISTORY': 14,\n",
    "    'PASTMEDICALHX': 15,\n",
    "    'PASTSURGICAL': 16,\n",
    "    'PLAN': 17,\n",
    "    'PROCEDURES': 18,\n",
    "    'ROS': 19\n",
    "}\n",
    "\n",
    "# Apply label dictionary to both datasets\n",
    "train_data['labels'] = train_data['labels'].map(label_dict)\n",
    "val_data['labels'] = val_data['labels'].map(label_dict)\n",
    "test_data['labels'] = test_data['labels'].map(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d8c1e5-f56b-4797-9cbb-90d027ba56d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e4a5e5-f24c-4ebf-8dfd-9bbd7a3316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Function to plot histogram\n",
    "# def plot_label_distribution(data, title):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.hist(data['labels'], bins=len(data['labels'].unique()), alpha=0.7, color='blue', edgecolor='black')\n",
    "#     plt.title(f'Label Distribution in {title}')\n",
    "#     plt.xlabel('Labels')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10a6ce3a-a545-44fa-ba7c-07b1dbf8432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82bb7f3e-ae70-4028-9e19-f1cdde14b84e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b88ca-b5d3-48eb-8f29-c285a7cb3c74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ab391c5-1c75-4749-8345-89c659b05841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for the training data (optional, test with and without)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_data['labels']), y=train_data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9b51408-655b-45e1-b127-0d56f1b2467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10415c-2436-42a2-bbee-7a7e89dfda85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f11e4-309e-46a5-b2d9-a26d86c9ce28",
   "metadata": {},
   "source": [
    "### <font color = grey> CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c545dbf-15a6-41aa-b40d-a428c93e43d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gaurav_narasimhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    # text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "train_data.dropna(inplace=True)\n",
    "val_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "\n",
    "# Applying the preprocessing function\n",
    "train_data['text'] = train_data['text'].apply(preprocess_text)\n",
    "val_data['text'] = val_data['text'].apply(preprocess_text)\n",
    "test_data['text'] = test_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd03d18d-fcf1-40fa-87bc-6062f09d7f7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3534fba-3a13-4476-b04e-56bd14695b6e",
   "metadata": {},
   "source": [
    "## <font color = Goldenrod> Keyword-Based Classification Before Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8775dee6-4635-466c-a0b3-e06787f65aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define strong indicator words for each section\n",
    "strong_indicator_words = {\n",
    "    # 'GYNHX': ['pregnant', 'miscarriage', 'abortion', 'periods', 'menstrual', 'obstetrician', 'gyneco'],\n",
    "    # 'IMMUNIZATIONS': ['vaccine', 'immunization'],\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "# Function to classify based on keywords\n",
    "def keyword_based_classification(text, label_dict):\n",
    "    for label, keywords in strong_indicator_words.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return label_dict[label]\n",
    "    return None\n",
    "\n",
    "# Apply keyword classification to all datasets\n",
    "train_data['keyword_label'] = train_data['text'].apply(lambda x: keyword_based_classification(x, label_dict))\n",
    "val_data['keyword_label'] = val_data['text'].apply(lambda x: keyword_based_classification(x, label_dict))\n",
    "test_data['keyword_label'] = test_data['text'].apply(lambda x: keyword_based_classification(x, label_dict))\n",
    "\n",
    "# Filter out the data already classified by keywords\n",
    "train_data = train_data[train_data['keyword_label'].isnull()]\n",
    "val_data = val_data[val_data['keyword_label'].isnull()]\n",
    "test_data = test_data[test_data['keyword_label'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6fd504-a248-43d7-ae55-30067d019084",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6106a0-61ab-409f-9fd7-1a5b7096148f",
   "metadata": {},
   "source": [
    "### <font color = grey> TOKENIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf729880-d040-46d6-aa55-5bdf95dea514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "\n",
    "# Use dynamic padding\n",
    "train_encodings = tokenizer(train_data['text'].tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_data['text'].tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_data['text'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_encodings, train_data['labels'].tolist())\n",
    "val_dataset = TextDataset(val_encodings, val_data['labels'].tolist())\n",
    "test_dataset = TextDataset(test_encodings, test_data['labels'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe63c8c-b1d1-43f6-a575-2705895a427e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37396a-c975-47c3-8bbe-208f6fc87bb3",
   "metadata": {},
   "source": [
    "# <font color = blue> Part 2: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77adb083-641a-4e64-9467-0fc02c6a51be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c29745-8afa-446a-9621-f1b88162f5ab",
   "metadata": {},
   "source": [
    "## <font color = Goldenrod> Step 1: Set Up Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badb6342-b7ac-4783-b033-b96450303e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='zibajoon/20231127_Roberta_30ep_NewLabels',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=28,  # Adjust batch size based on your GPU\n",
    "    gradient_accumulation_steps=2,  # Adjust as needed\n",
    "    report_to=\"none\",\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-05,  # Adjust learning rate\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"eval_loss\", #\"accuracy\",  # Change metric to accuracy\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc5666-1032-4eeb-ab45-f3b8bde2f626",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77587dc-c2ba-4a21-8c60-56e56acfa194",
   "metadata": {},
   "source": [
    "## <font color = Goldenrod> Step 2: Address Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99980ce9-e001-4c1e-90d6-014447869047",
   "metadata": {},
   "source": [
    "#### <font color = grey> ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "475195dc-b135-4600-acd7-0d34c36c8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use if class weights improved performance\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(training_args.device)\n",
    "criterion = CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b08600-a75e-47aa-9152-0cf883c5377a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfda8e6-acbf-478c-8cc4-7c9e43aaf4d9",
   "metadata": {},
   "source": [
    "## <font color = Goldenrod> Step 3: Initialize the Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca4189cd-24de-4905-b8b4-3cf1c3da9435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model initialization with pretrained weights\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(unique_labels))\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb4dc6-a7e1-4c1b-822c-93ae56e6674d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad7f74-7e46-4850-a973-b4083ad8abcc",
   "metadata": {},
   "source": [
    "## <font color = Goldenrod> Step 4: Initialize the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24b473b9-4fa1-44ae-b198-a508af969270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, get_linear_schedule_with_warmup\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        self.lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, \n",
    "            num_warmup_steps=0,  # You can change this if needed\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "\n",
    "    # def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    #     labels = inputs.get(\"labels\")\n",
    "    #     outputs = model(**inputs)\n",
    "    #     logits = outputs.get('logits')\n",
    "    #     loss = criterion(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "    #     return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Initialize the custom trainer with your model and arguments\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dab89f-7823-418b-88cf-4693ad6e4eba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009712ff-04dd-4542-b16e-3c39c561ae8c",
   "metadata": {},
   "source": [
    "## <font color = Goldenrod> Step 5: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "789066b4-38c3-4ef7-ab2b-fc1649dcb871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1290' max='1290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1290/1290 16:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.625526</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.142081</td>\n",
       "      <td>0.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.019819</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.087085</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.110796</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.019098</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.326718</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.973634</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.374900</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.237602</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.127393</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.314248</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.450431</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.421184</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.467404</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.469077</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.551787</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.519327</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.599726</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.564916</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.583198</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.597026</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>1.606632</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.609629</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.613492</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.617227</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.621582</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.624720</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.626412</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.626914</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1290, training_loss=0.17006556169931278, metrics={'train_runtime': 1008.3651, 'train_samples_per_second': 35.731, 'train_steps_per_second': 1.279, 'total_flos': 9481423414763520.0, 'train_loss': 0.17006556169931278, 'epoch': 30.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c030685-1e5c-426d-b866-a8247d7a639c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da88dc0f-2010-4734-b02e-e6d86e75712f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b9344bc63f4d6a93b62a0bce081422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f6f37c567548dcafeca38fb9c870b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b8236acc21419688cf738d73cf4bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/zibajoon/20231127_Roberta_30ep_NewLabels/tree/main/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.create_model_card()\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391046fb-f91d-4c8a-ab8e-3421827c0e1d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf83b06-9b19-4a9f-966f-da08d5672cd7",
   "metadata": {},
   "source": [
    "# <font color = green> Default Model - Original Inference & Metrics Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbabb4c-aaf7-4db0-b22b-697280b17a94",
   "metadata": {},
   "source": [
    "## <font color = Goldenrod> Step 1: Run Inference on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cd19476-08d1-49f5-9212-f71985ae5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference_with_keywords_and_model(model, dataset, original_data, device='cuda'):\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    model.to(device)\n",
    "    dataset_idx = 0  # Separate index for the dataset\n",
    "\n",
    "    # Iterate through the original dataset\n",
    "    for idx, row in original_data.iterrows():\n",
    "        true_labels.append(row['labels'])\n",
    "        if pd.notnull(row['keyword_label']):\n",
    "            # Use keyword-based classification\n",
    "            predictions.append(row['keyword_label'])\n",
    "        else:\n",
    "            # Use the model for prediction\n",
    "            item = dataset[dataset_idx]\n",
    "            with torch.no_grad():\n",
    "                input_ids = item['input_ids'].unsqueeze(0).to(device)\n",
    "                attention_mask = item['attention_mask'].unsqueeze(0).to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                predictions.append(torch.argmax(logits, dim=1).cpu().numpy()[0])\n",
    "            dataset_idx += 1  # Increment index for items that require model inference\n",
    "\n",
    "    return predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "825da937-004d-40ed-9d5e-fc240ac3a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "model.eval()\n",
    "test_predictions, test_true_labels = perform_inference_with_keywords_and_model(model, test_dataset, test_data, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ee0b9-083a-48c6-ae1f-8ea8623a88e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bde057-2b6e-4f28-918c-55d504e14878",
   "metadata": {},
   "source": [
    "## <font color = Goldenrod> Step 2: Calculate the Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb75d2c6-27aa-4b05-9bb2-b9815fa7481d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.76'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "from IPython.display import display\n",
    "\n",
    "accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "\n",
    "display(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0991b9-28b3-4945-bb42-353b7d53fe5a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6e63d-637b-4bf0-9553-1ca22d560f7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902877e-16d3-43c9-b407-f20f8b656613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "20231119_MediQA",
   "language": "python",
   "name": "20231119_mediqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
