{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c42915-006c-4b89-b37e-ffaa9ed18df7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# <font color = orange> MediQA --> Section Header --> <font color = teal> Clinical Longformer --> Inference from HF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e758657d-103c-4538-abea-351953529cd2",
   "metadata": {},
   "source": [
    "# <font color = tomato> STARTOVER_SIMPLE --> FIXED ISSUE WITH DICTIONARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab75672-987a-4345-9350-ace0b403ce7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2af10e8-a3c9-41a7-9d22-e64750283d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv.main import load_dotenv\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b2e822-6513-4bda-9c4b-6459c53a246a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02411b09-4104-4e1d-a056-d5c8cf6d92e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/20231116_MediQA/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "# Set to display full (non-truncated) dataframe information\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89864f1d-ab0d-4591-96cb-d078ebc7e977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from transformers import set_seed\n",
    "\n",
    "# # Set the seed value\n",
    "# seed_value = 1234\n",
    "\n",
    "# # Set seed for Python's random module\n",
    "# random.seed(seed_value)\n",
    "\n",
    "# # Set seed for NumPy\n",
    "# np.random.seed(seed_value)\n",
    "\n",
    "# # Set seed for PyTorch\n",
    "# torch.manual_seed(seed_value)\n",
    "\n",
    "# # If using CUDA (for GPU computations)\n",
    "# torch.cuda.manual_seed(seed_value)\n",
    "# torch.cuda.manual_seed_all(seed_value)  # For multi-GPU, if applicable\n",
    "\n",
    "# # Ensure deterministic behavior in PyTorch (may impact performance)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# # Set seed using Transformers' utility function (affects some specific random aspects in Transformers)\n",
    "# set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a340a9-a906-4f2a-9bd0-96208b9d4620",
   "metadata": {},
   "source": [
    "#### <font color = grey> Huggingface Token / Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc3f1cc-cf4e-4020-96b1-90bab854d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# Set your Hugging Face API token as an environment variable\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Save the token using HfFolder\n",
    "HfFolder.save_token(os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e003343-9fbf-4596-8fdb-ad3c28e169ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # HF: 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\n",
    "# # # Open AI: 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\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91ab85-9ecd-4be2-be61-1d860e5b31a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6c6482-01d5-49a6-8838-4f65e848c9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 02:53:04.907556: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-28 02:53:04.907606: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-28 02:53:04.907628: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44c312-bba9-4b3c-a42b-ab9228dcbfa2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7b871-9a9a-402c-a959-25d168877f90",
   "metadata": {},
   "source": [
    "# STARTOVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cdaea96-7a2c-4875-a9de-a28828c71cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the test dataset\n",
    "test_data_path = '/home/gaurav_narasimhan/03.gn_projects/03.MediQA_2023/11.Source_Data/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a83be6bc-af26-42e2-85b4-e3b5d5554dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    0: \"GENHX\",\n",
    "    1: \"MEDICATIONS\",\n",
    "    2: \"CC\",\n",
    "    3: \"PASTMEDICALHX\",\n",
    "    4: \"ALLERGY\",\n",
    "    5: \"FAM/SOCHX\",\n",
    "    6: \"PASTSURGICAL\",\n",
    "    7: \"OTHER_HISTORY\",\n",
    "    8: \"ASSESSMENT\",\n",
    "    9: \"ROS\",\n",
    "    10: \"DISPOSITION\",\n",
    "    11: \"EXAM\",\n",
    "    12: \"PLAN\",\n",
    "    13: \"DIAGNOSIS\",\n",
    "    14: \"EDCOURSE\",\n",
    "    15: \"IMMUNIZATIONS\",\n",
    "    16: \"LABS\",\n",
    "    17: \"IMAGING\",\n",
    "    18: \"PROCEDURES\",\n",
    "    19: \"GYNHX\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8594bb95-b1d2-4e4c-8984-c4e08dcfee47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6345ff8e-929c-41bf-bf73-d04bc605454d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerForSequenceClassification(\n",
       "  (longformer): LongformerModel(\n",
       "    (embeddings): LongformerEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "    )\n",
       "    (encoder): LongformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): LongformerClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer for Clinical-Longformer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
    "\n",
    "# def tokenize_data(data):\n",
    "#     return tokenizer(\n",
    "#         data['text'].tolist(),\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=4096,  # Clinical-Longformer also supports longer sequences\n",
    "#         return_tensors='pt'\n",
    "#     )\n",
    "\n",
    "# Tokenize the dialogues\n",
    "encodings = tokenizer(\n",
    "    test_data['dialogue'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=4096,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Tokenize the test data\n",
    "# test_encodings = tokenize_data(test_data)\n",
    "\n",
    "# Load your own pretrained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"zibajoon/20231125_MediQA_Clinical_Longformer_8_epoch_LR_5e-05_BS_3_78_76.5\",\n",
    "    num_labels=20)\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5dfa328-52dc-44a1-8f1f-cdd99a831488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initialize the tokenizer and the model\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForSequenceClassification.from_pretrained(\"zibajoon/20231126_MediQA_Baseline_BERT_10_epoch_LR_5e-05_BS_3_77_77\", num_labels=20)\n",
    "# model.eval()\n",
    "\n",
    "# # Tokenize the dialogues\n",
    "# encodings = tokenizer(\n",
    "#     test_data['dialogue'].tolist(),\n",
    "#     padding=True,\n",
    "#     truncation=True,\n",
    "#     max_length=512,\n",
    "#     return_tensors='pt'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "becf51a4-f31d-4b12-9820-12a0f49acb71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_with_debug(model, encodings, actual_labels):\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(encodings['input_ids'].size(0)):\n",
    "            inputs = {key: val[i].unsqueeze(0) for key, val in encodings.items()}\n",
    "            outputs = model(**inputs)\n",
    "            pred_label_idx = torch.argmax(outputs.logits, dim=1).item()\n",
    "            pred_label_name = label_dict.get(pred_label_idx, \"Unknown Label\")\n",
    "            predicted_labels.append(pred_label_name)\n",
    "            \n",
    "            # Print debugging information\n",
    "            actual_label_name = actual_labels[i]\n",
    "            print(f\"Index: {i}, Predicted Label: {pred_label_name}, Actual Label: {actual_label_name}\")\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9dd9d9-14b2-44e6-b4b3-efcea73ac6c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Predicted Label: GENHX, Actual Label: GENHX\n",
      "Index: 1, Predicted Label: FAM/SOCHX, Actual Label: FAM/SOCHX\n",
      "Index: 2, Predicted Label: ROS, Actual Label: ROS\n",
      "Index: 3, Predicted Label: FAM/SOCHX, Actual Label: FAM/SOCHX\n",
      "Index: 4, Predicted Label: FAM/SOCHX, Actual Label: FAM/SOCHX\n",
      "Index: 5, Predicted Label: PASTMEDICALHX, Actual Label: FAM/SOCHX\n",
      "Index: 6, Predicted Label: CC, Actual Label: GENHX\n",
      "Index: 7, Predicted Label: FAM/SOCHX, Actual Label: FAM/SOCHX\n",
      "Index: 8, Predicted Label: FAM/SOCHX, Actual Label: FAM/SOCHX\n",
      "Index: 9, Predicted Label: ALLERGY, Actual Label: ALLERGY\n"
     ]
    }
   ],
   "source": [
    "# Extract the actual labels from the test data\n",
    "actual_labels = test_data['section_header'].tolist()\n",
    "\n",
    "# Perform inference with debugging\n",
    "predicted_labels = predict_with_debug(model, encodings, actual_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fbab93-a21a-4429-aad8-a9ccc2b1d7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "20231119_MediQA",
   "language": "python",
   "name": "20231119_mediqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
